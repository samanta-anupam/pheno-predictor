{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Population predictor\n",
    "=============\n",
    "\n",
    "Using Deep Learning\n",
    "------------\n",
    "\n",
    "The following things are being done here.\n",
    "1. Load dataset.npy file for the TPM count from quant.sf file. If not present:\n",
    "    a. Read from the quant.sf files to a numpy array dataset.\n",
    "    b. Save it using numpy.save file for faster access.\n",
    "2. Perform Dimensionality reduction. Possible methods:\n",
    "    a. PCA (Problems: Not for dataset with number of features larger than number of samples).\n",
    "    b. TSNE (Slow)\n",
    "    c. Encoder (Deep learning approach to encode into a lower dimensional form)\n",
    "3. Apply the model to this reduced feature vector. Possible models:\n",
    "    a. Fully connected neural network with softmax loss.\n",
    "    b. Conv1D\n",
    "    c. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import yadlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the input from the quant.sf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 0 3 1 2 1 3 1 1 4 4 1 1 2 2 0 0 1 3 2 1 1 4 0 0 4 3 4 0 3 1 4 4 2 2\n",
      " 1 0 1 1 3 3 3 2 4 3 2 4 2 4 0 1 4 0 4 4 1 2 1 1 3 4 2 1 3 4 0 2 2 4 0 0 4\n",
      " 0 4 2 0 0 0 0 1 0 1 0 3 3 2 3 0 0 1 3 2 2 1 0 2 4 4 1 2 0 1 3 4 4 2 4 4 3\n",
      " 0 2 2 3 3 1 3 3 3 4 2 2 3 0 0 2 0 3 2 2 0 2 1 0 0 3 2 3 1 4 2 4 3 2 4 2 3\n",
      " 4 2 1 0 0 4 2 1 3 2 0 3 1 0 2 2 1 3 4 4 3 3 3 2 2 2 1 4 2 2 0 0 1 3 3 0 3\n",
      " 3 4 0 0 1 0 2 4 2 3 1 2 0 1 1 1 0 2 4 2 2 4 2 0 4 4 1 2 3 3 2 3 2 0 4 0 3\n",
      " 4 3 2 1 4 0 1 1 3 4 1 3 0 3 4 4 2 4 3 4 3 3 3 3 3 3 3 0 4 1 1 4 2 0 0 4 0\n",
      " 2 1 4 1 1 4 2 2 2 3 0 1 0 3 0 0 4 1 1 2 4 0 1 4 0 0 0 3 1 2 4 1 4 4 1 3 1\n",
      " 0 1 1 4 2 2 2 1 1 4 3 2 3 4 2 1 3 4 4 4 1 4 3 1 0 2 3 3 0 2 3 1 1 2 1 3 4\n",
      " 0 3 4 1 4 0 3 4 0 2 1 1 1 3 3 0 3 0 3 1 0 1 3 4 2 1 3 1 1 1 3 1 4 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file to map samples vs labels.\n",
    "data_frame = pd.read_csv('p1_train.csv')\n",
    "labels = list(set(data_frame['label']))\n",
    "\n",
    "num_of_transcripts = 199324\n",
    "num_of_samples = data_frame.shape[0]\n",
    "num_labels = len(labels)\n",
    "\n",
    "dataset = np.empty((num_of_samples, num_of_transcripts), dtype=np.float32)\n",
    "sample_vs_label = np.empty(num_of_samples, dtype='int')\n",
    "samples = np.empty(num_of_samples, dtype='<U9')\n",
    "\n",
    "for index, row in data_frame.iterrows():\n",
    "    if(index >= num_of_samples): break\n",
    "    sample_vs_label[index] = labels.index(row['label'])\n",
    "    samples[index] = row['accession']\n",
    "print(sample_vs_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and read from any saved 'dataset.npy' file. If present then load it to numpy array dataset, else just read it from all possible folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.23 ms, sys: 235 ms, total: 236 ms\n",
      "Wall time: 289 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if(os.path.isfile('dataset.npy')):\n",
    "    dataset = np.load('dataset.npy')\n",
    "else:\n",
    "    TRAIN_PATH = './train/'\n",
    "    # Read the 'quant.sf' value here for each sample.\n",
    "    for i in range(num_of_samples):\n",
    "        if i%20==0: print(i)\n",
    "        file_name = TRAIN_PATH + samples[i] + '/bias/quant.sf'\n",
    "        quant_sf = np.genfromtxt(file_name, delimiter='\\t', usecols=3, skip_header=True, dtype=np.float32)\n",
    "        dataset[i] = quant_sf\n",
    "\n",
    "    # Using https://i.stack.imgur.com/4d6yo.png to judge the best way to save the dataset as a npy file\n",
    "    # for faster reloading of numpy array.\n",
    "    np.save('dataset.npy', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199324"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(dataset, axis=0).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VarianceThreshold\n",
    "Simple baseline approach to feature selection. It removes all features whose variance doesnâ€™t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples.\n",
    "http://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369, 64843)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "dataset_reduced = sel.fit_transform(dataset)\n",
    "\n",
    "dataset_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-based feature selection\n",
    "\n",
    "Tree-based estimators (see the sklearn.tree module and forest of trees in the sklearn.ensemble module) can be used to compute feature importances, which in turn can be used to discard irrelevant features (when coupled with the sklearn.feature_selection.SelectFromModel meta-transformer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369, 1011)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ExtraTreesClassifier()\n",
    "clf = clf.fit(dataset[:int(0.9*dataset.size)], sample_vs_label[:int(0.9*dataset.size)])\n",
    "clf.feature_importances_  \n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "dataset_reduced = model.transform(dataset)\n",
    "dataset_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying PCA\n",
    "## results = Test accuracy: 42.1%\n",
    "\n",
    "Problem: Cannot reduce to dimensions greater than number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_of_transcripts = 300\n",
    "pca = PCA(n_components=num_of_transcripts)\n",
    "pca.fit(std_dataset)\n",
    "dataset_reduced = pca.transform(std_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color_mapping = [sns.xkcd_rgb['blue'], sns.xkcd_rgb['lime'], sns.xkcd_rgb['ochre'], \n",
    "                 sns.xkcd_rgb['red'], sns.xkcd_rgb['green']]\n",
    "\n",
    "colors = [color_mapping[x] for x in sample_vs_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(dataset_reduced[:, 0], dataset_reduced[:, 10], c=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying TSNE\n",
    "## (TODO)\n",
    "Under construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = TSNE(learning_rate=100, n_components=20, random_state=0, perplexity=10)\n",
    "tsne5 = model.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Stacked Denoising Autoencoder \n",
    "## from yadlt \n",
    "https://github.com/blackecho/Deep-Learning-TensorFlow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sda = yadlt.SDAutoencoder(dims=[784, 400, 200, 80],\n",
    "                    activations=[\"sigmoid\", \"sigmoid\", \"sigmoid\"],\n",
    "                    sess=sess,\n",
    "                    noise=0.20,\n",
    "                    loss=\"cross-entropy\",\n",
    "                    pretrain_lr=0.0001,\n",
    "                    finetune_lr=0.0001)\n",
    "sda.fit(dataset[:int(0.9*num_of_samples)])\n",
    "dataset_reduced = sda.transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main modeling\n",
    "## model data to machine learning tools\n",
    "Split data to different datasets, namely, train, validation, test. Currently splitting in 0.8, 0.1, 0.1 ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (295, 1011) (295,)\n",
      "Validation: (36, 1011) (36,)\n",
      "Testing: (38, 1011) (38,)\n"
     ]
    }
   ],
   "source": [
    "def merge_datasets(dataset, train_size, valid_size, test_size):\n",
    "  valid_dataset, valid_labels = dataset[:valid_size], sample_vs_label[:valid_size]\n",
    "  train_dataset, train_labels = dataset[valid_size:valid_size+train_size], sample_vs_label[valid_size:valid_size+train_size]\n",
    "  test_dataset, test_labels = dataset[valid_size+train_size:], sample_vs_label[valid_size+train_size:]\n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels, test_dataset, test_labels\n",
    "\n",
    "train_size = int(0.8*num_of_samples)\n",
    "valid_size = int(0.1*num_of_samples)\n",
    "test_size = num_of_samples-valid_size-train_size\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels, test_dataset, test_labels = merge_datasets(dataset_reduced, train_size, valid_size, test_size)\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1011"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use One Hot encoding for the labels here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "output_extras": [
      {
       "item_id": 1.0
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11952.0,
     "status": "ok",
     "timestamp": 1.446658914857E12,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480.0
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (295, 1011) (295, 5)\n",
      "Validation set (36, 1011) (36, 5)\n",
      "Test set (38, 1011) (38, 5)\n"
     ]
    }
   ],
   "source": [
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, dataset.shape[1])).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369, 1011)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "## FC Connected layer\n",
    "Using a 2-layer fully connected neural network of 128, 64 relu neurons with batch size of 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 128)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_hidden_nodes = [128, 64]\n",
    "num_of_transcripts = train_dataset.shape[1]\n",
    "input_size = [num_of_transcripts]+num_hidden_nodes\n",
    "num_layers = len(num_hidden_nodes)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=\n",
    "                                     (batch_size, num_of_transcripts))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=\n",
    "                                    (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    # Weights\n",
    "    weights1 = []\n",
    "    biases1 = []\n",
    "    beta1 = []\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        weights1.append(tf.Variable(tf.truncated_normal([input_size[i], num_hidden_nodes[i]],\\\n",
    "                                                        stddev=np.sqrt(2.0 / (input_size[i])))))\n",
    "        biases1.append(tf.Variable(tf.zeros([num_hidden_nodes[i]])))\n",
    "        beta1.append(1e-3)\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([input_size[-1], num_labels],\\\n",
    "                                               stddev=np.sqrt(2.0 / input_size[-1])))\n",
    "    beta2 = 1e-3\n",
    "    \n",
    "    # Training Computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1[0]) + biases1[0])\n",
    "    drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "    for i in range(1, num_layers):\n",
    "        print(lay1_train.shape)\n",
    "        lay1_train = tf.nn.relu(tf.matmul(drop1, weights1[i]) + biases1[i])\n",
    "        drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "    logits = tf.matmul(drop1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\\\n",
    "                           + beta2 * tf.nn.l2_loss(weights2)\n",
    "    for i in range(num_layers):\n",
    "        loss += beta1[i] * tf.nn.l2_loss(weights1[i]) \n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate = tf.train.exponential_decay(1e-4, global_step, 1000000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "                           \n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1[0]) + biases1[0])\n",
    "    for i in range(1, num_layers):\n",
    "        lay1_valid = tf.nn.relu(tf.matmul(lay1_valid, weights1[i]) + biases1[i])\n",
    "    logits_valid = tf.matmul(lay1_valid, weights2) + biases2\n",
    "    valid_prediction = tf.nn.softmax(logits_valid)\n",
    "                           \n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1[0]) + biases1[0])\n",
    "    for i in range(1, num_layers):\n",
    "        lay1_test = tf.nn.relu(tf.matmul(lay1_test, weights1[i]) + biases1[i])\n",
    "    logits_test = tf.matmul(lay1_test, weights2) + biases2\n",
    "    test_prediction = tf.nn.softmax(logits_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the neural net\n",
    "## Training for 18001 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised\n",
      "Minibatch loss at step 0: 283.436737\n",
      "Minibatch accuracy: 0.0% \n",
      "Validation accuracy: 22.2% \n",
      "Minibatch loss at step 500: 88.532761\n",
      "Minibatch accuracy: 12.5% \n",
      "Validation accuracy: 25.0% \n",
      "Minibatch loss at step 1000: 6.661283\n",
      "Minibatch accuracy: 37.5% \n",
      "Validation accuracy: 36.1% \n",
      "Minibatch loss at step 1500: 11.462114\n",
      "Minibatch accuracy: 37.5% \n",
      "Validation accuracy: 44.4% \n",
      "Minibatch loss at step 2000: 1.488258\n",
      "Minibatch accuracy: 50.0% \n",
      "Validation accuracy: 50.0% \n",
      "Minibatch loss at step 2500: 1.385663\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 41.7% \n",
      "Minibatch loss at step 3000: 2.148892\n",
      "Minibatch accuracy: 25.0% \n",
      "Validation accuracy: 41.7% \n",
      "Minibatch loss at step 3500: 1.658631\n",
      "Minibatch accuracy: 12.5% \n",
      "Validation accuracy: 50.0% \n",
      "Minibatch loss at step 4000: 1.235050\n",
      "Minibatch accuracy: 50.0% \n",
      "Validation accuracy: 44.4% \n",
      "Minibatch loss at step 4500: 1.574324\n",
      "Minibatch accuracy: 25.0% \n",
      "Validation accuracy: 50.0% \n",
      "Minibatch loss at step 5000: 1.521829\n",
      "Minibatch accuracy: 50.0% \n",
      "Validation accuracy: 47.2% \n",
      "Minibatch loss at step 5500: 0.982024\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 47.2% \n",
      "Minibatch loss at step 6000: 1.207639\n",
      "Minibatch accuracy: 37.5% \n",
      "Validation accuracy: 47.2% \n",
      "Minibatch loss at step 6500: 1.107589\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 50.0% \n",
      "Minibatch loss at step 7000: 0.953392\n",
      "Minibatch accuracy: 50.0% \n",
      "Validation accuracy: 47.2% \n",
      "Minibatch loss at step 7500: 0.945596\n",
      "Minibatch accuracy: 50.0% \n",
      "Validation accuracy: 47.2% \n",
      "Minibatch loss at step 8000: 1.571648\n",
      "Minibatch accuracy: 37.5% \n",
      "Validation accuracy: 50.0% \n",
      "Minibatch loss at step 8500: 0.893600\n",
      "Minibatch accuracy: 75.0% \n",
      "Validation accuracy: 58.3% \n",
      "Minibatch loss at step 9000: 1.013434\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 69.4% \n",
      "Minibatch loss at step 9500: 0.828206\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 61.1% \n",
      "Minibatch loss at step 10000: 0.520770\n",
      "Minibatch accuracy: 75.0% \n",
      "Validation accuracy: 66.7% \n",
      "Minibatch loss at step 10500: 0.676840\n",
      "Minibatch accuracy: 87.5% \n",
      "Validation accuracy: 66.7% \n",
      "Minibatch loss at step 11000: 1.058789\n",
      "Minibatch accuracy: 75.0% \n",
      "Validation accuracy: 66.7% \n",
      "Minibatch loss at step 11500: 1.042573\n",
      "Minibatch accuracy: 25.0% \n",
      "Validation accuracy: 66.7% \n",
      "Minibatch loss at step 12000: 1.021556\n",
      "Minibatch accuracy: 75.0% \n",
      "Validation accuracy: 63.9% \n",
      "Minibatch loss at step 12500: 0.987414\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 63.9% \n",
      "Minibatch loss at step 13000: 1.052208\n",
      "Minibatch accuracy: 50.0% \n",
      "Validation accuracy: 75.0% \n",
      "Minibatch loss at step 13500: 0.655157\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 69.4% \n",
      "Minibatch loss at step 14000: 1.151415\n",
      "Minibatch accuracy: 37.5% \n",
      "Validation accuracy: 63.9% \n",
      "Minibatch loss at step 14500: 0.762213\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 66.7% \n",
      "Minibatch loss at step 15000: 0.489183\n",
      "Minibatch accuracy: 75.0% \n",
      "Validation accuracy: 63.9% \n",
      "Minibatch loss at step 15500: 0.450497\n",
      "Minibatch accuracy: 75.0% \n",
      "Validation accuracy: 63.9% \n",
      "Minibatch loss at step 16000: 0.923069\n",
      "Minibatch accuracy: 75.0% \n",
      "Validation accuracy: 69.4% \n",
      "Minibatch loss at step 16500: 1.142923\n",
      "Minibatch accuracy: 50.0% \n",
      "Validation accuracy: 63.9% \n",
      "Minibatch loss at step 17000: 0.704360\n",
      "Minibatch accuracy: 50.0% \n",
      "Validation accuracy: 69.4% \n",
      "Minibatch loss at step 17500: 0.722631\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 63.9% \n",
      "Minibatch loss at step 18000: 0.634654\n",
      "Minibatch accuracy: 75.0% \n",
      "Validation accuracy: 69.4% \n",
      "Test accuracy: 65.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "accuracy_val = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialised')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % \\\n",
    "                (train_labels.shape[0] - batch_size)\n",
    "\n",
    "        batch_data = train_dataset[offset:offset+batch_size, :]\n",
    "        batch_labels = train_labels[offset:offset+batch_size, :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset : batch_data, \\\n",
    "                     tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, \\\n",
    "                                         train_prediction], feed_dict=feed_dict)\n",
    "        if step%500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%% \" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%% \" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
