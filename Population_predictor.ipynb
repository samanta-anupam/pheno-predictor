{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Population predictor\n",
    "=============\n",
    "\n",
    "Using Deep Learning\n",
    "------------\n",
    "\n",
    "The following things are being done here.\n",
    "1. Load dataset.npy file for the TPM count from quant.sf file. If not present:\n",
    "    a. Read from the quant.sf files to a numpy array dataset.\n",
    "    b. Save it using numpy.save file for faster access.\n",
    "2. Perform Dimensionality reduction. Possible methods:\n",
    "    a. PCA (Problems: Not for dataset with number of features larger than number of samples).\n",
    "    b. TSNE (Slow)\n",
    "    c. Encoder (Deep learning approach to encode into a lower dimensional form)\n",
    "3. Apply the model to this reduced feature vector. Possible models:\n",
    "    a. Fully connected neural network with softmax loss.\n",
    "    b. Conv1D\n",
    "    c. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import yadlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the input from the quant.sf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 0 2 3 4 3 2 3 3 1 1 3 3 4 4 0 0 3 2 4 3 3 1 0 0 1 2 1 0 2 3 1 1 4 4\n",
      " 3 0 3 3 2 2 2 4 1 2 4 1 4 1 0 3 1 0 1 1 3 4 3 3 2 1 4 3 2 1 0 4 4 1 0 0 1\n",
      " 0 1 4 0 0 0 0 3 0 3 0 2 2 4 2 0 0 3 2 4 4 3 0 4 1 1 3 4 0 3 2 1 1 4 1 1 2\n",
      " 0 4 4 2 2 3 2 2 2 1 4 4 2 0 0 4 0 2 4 4 0 4 3 0 0 2 4 2 3 1 4 1 2 4 1 4 2\n",
      " 1 4 3 0 0 1 4 3 2 4 0 2 3 0 4 4 3 2 1 1 2 2 2 4 4 4 3 1 4 4 0 0 3 2 2 0 2\n",
      " 2 1 0 0 3 0 4 1 4 2 3 4 0 3 3 3 0 4 1 4 4 1 4 0 1 1 3 4 2 2 4 2 4 0 1 0 2\n",
      " 1 2 4 3 1 0 3 3 2 1 3 2 0 2 1 1 4 1 2 1 2 2 2 2 2 2 2 0 1 3 3 1 4 0 0 1 0\n",
      " 4 3 1 3 3 1 4 4 4 2 0 3 0 2 0 0 1 3 3 4 1 0 3 1 0 0 0 2 3 4 1 3 1 1 3 2 3\n",
      " 0 3 3 1 4 4 4 3 3 1 2 4 2 1 4 3 2 1 1 1 3 1 2 3 0 4 2 2 0 4 2 3 3 4 3 2 1\n",
      " 0 2 1 3 1 0 2 1 0 4 3 3 3 2 2 0 2 0 2 3 0 3 2 1 4 3 2 3 3 3 2 3 1 4 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file to map samples vs labels.\n",
    "data_frame = pd.read_csv('p1_train.csv')\n",
    "labels = list(set(data_frame['label']))\n",
    "\n",
    "num_of_transcripts = 199324\n",
    "num_of_samples = data_frame.shape[0]\n",
    "num_labels = len(labels)\n",
    "\n",
    "dataset = np.empty((num_of_samples, num_of_transcripts), dtype=np.float32)\n",
    "sample_vs_label = np.empty(num_of_samples, dtype='int')\n",
    "samples = np.empty(num_of_samples, dtype='<U9')\n",
    "\n",
    "for index, row in data_frame.iterrows():\n",
    "    if(index >= num_of_samples): break\n",
    "    sample_vs_label[index] = labels.index(row['label'])\n",
    "    samples[index] = row['accession']\n",
    "print(sample_vs_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and read from any saved 'dataset.npy' file. If present then load it to numpy array dataset, else just read it from all possible folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.59 ms, sys: 216 ms, total: 218 ms\n",
      "Wall time: 229 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if(os.path.isfile('dataset.npy')):\n",
    "    dataset = np.load('dataset.npy')\n",
    "else:\n",
    "    TRAIN_PATH = './train/'\n",
    "    # Read the 'quant.sf' value here for each sample.\n",
    "    for i in range(num_of_samples):\n",
    "        if i%20==0: print(i)\n",
    "        file_name = TRAIN_PATH + samples[i] + '/bias/quant.sf'\n",
    "        quant_sf = np.genfromtxt(file_name, delimiter='\\t', usecols=3, skip_header=True, dtype=np.float32)\n",
    "        dataset[i] = quant_sf\n",
    "\n",
    "    # Using https://i.stack.imgur.com/4d6yo.png to judge the best way to save the dataset as a npy file\n",
    "    # for faster reloading of numpy array.\n",
    "    np.save('dataset.npy', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VarianceThreshold\n",
    "Simple baseline approach to feature selection. It removes all features whose variance doesnâ€™t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples.\n",
    "http://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369, 64843)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "dataset_reduced = sel.fit_transform(dataset)\n",
    "\n",
    "dataset_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-based feature selection\n",
    "\n",
    "Tree-based estimators (see the sklearn.tree module and forest of trees in the sklearn.ensemble module) can be used to compute feature importances, which in turn can be used to discard irrelevant features (when coupled with the sklearn.feature_selection.SelectFromModel meta-transformer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369, 1037)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "scaler = StandardScaler().fit(dataset)\n",
    "dataset_reduced = scaler.transform(dataset)\n",
    "\n",
    "clf = ExtraTreesClassifier()\n",
    "clf = clf.fit(dataset_reduced[:int(dataset.size)], sample_vs_label[:int(dataset.size)])\n",
    "clf.feature_importances_  \n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "dataset_reduced = model.transform(dataset_reduced)\n",
    "dataset_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying PCA\n",
    "## results = Test accuracy: 42.1%\n",
    "\n",
    "Problem: Cannot reduce to dimensions greater than number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'std_dataset' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8750287e0e8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_of_transcripts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_of_transcripts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdataset_reduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'std_dataset' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "num_of_transcripts = 300\n",
    "pca = PCA(n_components=num_of_transcripts)\n",
    "pca.fit(std_dataset)\n",
    "dataset_reduced = pca.transform(std_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color_mapping = [sns.xkcd_rgb['blue'], sns.xkcd_rgb['lime'], sns.xkcd_rgb['ochre'], \n",
    "                 sns.xkcd_rgb['red'], sns.xkcd_rgb['green']]\n",
    "\n",
    "colors = [color_mapping[x] for x in sample_vs_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(dataset_reduced[:, 0], dataset_reduced[:, 10], c=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying TSNE\n",
    "## (TODO)\n",
    "Under construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = TSNE(learning_rate=100, n_components=20, random_state=0, perplexity=10)\n",
    "tsne5 = model.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Stacked Denoising Autoencoder \n",
    "## from yadlt \n",
    "https://github.com/blackecho/Deep-Learning-TensorFlow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sda = yadlt.SDAutoencoder(dims=[784, 400, 200, 80],\n",
    "                    activations=[\"sigmoid\", \"sigmoid\", \"sigmoid\"],\n",
    "                    sess=sess,\n",
    "                    noise=0.20,\n",
    "                    loss=\"cross-entropy\",\n",
    "                    pretrain_lr=0.0001,\n",
    "                    finetune_lr=0.0001)\n",
    "sda.fit(dataset[:int(0.9*num_of_samples)])\n",
    "dataset_reduced = sda.transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main modeling\n",
    "## model data to machine learning tools\n",
    "Split data to different datasets, namely, train, validation, test. Currently splitting in 0.8, 0.1, 0.1 ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def merge_datasets(dataset, train_size, valid_size, test_size):\n",
    "  valid_dataset, valid_labels = dataset[:valid_size], sample_vs_label[:valid_size]\n",
    "  train_dataset, train_labels = dataset[valid_size:valid_size+train_size], sample_vs_label[valid_size:valid_size+train_size]\n",
    "  test_dataset, test_labels = dataset[valid_size+train_size:], sample_vs_label[valid_size+train_size:]\n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels, test_dataset, test_labels\n",
    "\n",
    "train_size = int(0.8*num_of_samples)\n",
    "valid_size = int(0.1*num_of_samples)\n",
    "test_size = num_of_samples-valid_size-train_size\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels, test_dataset, test_labels = merge_datasets(dataset_reduced, train_size, valid_size, test_size)\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_dataset.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use One Hot encoding for the labels here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "output_extras": [
      {
       "item_id": 1.0
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11952.0,
     "status": "ok",
     "timestamp": 1.446658914857E12,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480.0
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 0 2]\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def reformat(labels):\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return labels\n",
    "print(sample_vs_label[:5])\n",
    "sample_vs_label = reformat(sample_vs_label)\n",
    "print(sample_vs_label[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369, 1037)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "## FC Connected layer\n",
    "Using a 2-layer fully connected neural network of 128, 64 relu neurons with batch size of 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 1037)\n",
      "(8, 128)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_hidden_nodes = [128, 64]\n",
    "num_of_transcripts = dataset_reduced.shape[1]\n",
    "input_size = [num_of_transcripts]+num_hidden_nodes\n",
    "test_size = 74\n",
    "num_layers = len(num_hidden_nodes)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=\n",
    "                                     (batch_size, num_of_transcripts))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=\n",
    "                                    (batch_size, num_labels))\n",
    "    tf_test_data = tf.placeholder(tf.float32, shape=\n",
    "                                    (test_size, num_of_transcripts))\n",
    "    print(tf_test_data.shape)\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    # Weights\n",
    "    weights1 = []\n",
    "    biases1 = []\n",
    "    beta1 = []\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        weights1.append(tf.Variable(tf.truncated_normal([input_size[i], num_hidden_nodes[i]],\\\n",
    "                                                        stddev=np.sqrt(2.0 / (input_size[i])))))\n",
    "        biases1.append(tf.Variable(tf.zeros([num_hidden_nodes[i]])))\n",
    "        beta1.append(1e-3)\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([input_size[-1], num_labels],\\\n",
    "                                               stddev=np.sqrt(2.0 / input_size[-1])))\n",
    "    beta2 = 1e-3\n",
    "    \n",
    "    # Training Computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1[0]) + biases1[0])\n",
    "    drop1 = tf.nn.dropout(lay1_train, 0.2)\n",
    "    for i in range(1, num_layers):\n",
    "        print(lay1_train.shape)\n",
    "        lay1_train = tf.nn.relu(tf.matmul(drop1, weights1[i]) + biases1[i])\n",
    "        drop1 = tf.nn.dropout(lay1_train, 0.2)\n",
    "    logits = tf.matmul(drop1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\\\n",
    "                           + beta2 * tf.nn.l2_loss(weights2)\n",
    "    for i in range(num_layers):\n",
    "        loss += beta1[i] * tf.nn.l2_loss(weights1[i]) \n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate = tf.train.exponential_decay(1e-5, global_step, 1e6, 0.65, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "                           \n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_data, weights1[0]) + biases1[0])\n",
    "    for i in range(1, num_layers):\n",
    "        lay1_test = tf.nn.relu(tf.matmul(lay1_test, weights1[i]) + biases1[i])\n",
    "    logits_test = tf.matmul(lay1_test, weights2) + biases2\n",
    "    test_prediction = tf.nn.softmax(logits_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the neural net\n",
    "## Training for 18001 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised\n",
      "Test accuracy: 78.4%\n",
      "Test accuracy: 89.2%\n",
      "Test accuracy: 91.9%\n",
      "Test accuracy: 98.6%\n",
      "Test accuracy: 97.3%\n",
      "91.0811\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18000\n",
    "result = np.zeros(5, dtype=np.float32)\n",
    "i=0\n",
    "accuracy_val = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialised')\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "    for train_index, test_index in sss.split(dataset_reduced, sample_vs_label):\n",
    "        train_dataset, test_dataset = dataset_reduced[train_index], dataset_reduced[test_index]\n",
    "        train_labels, test_labels = sample_vs_label[train_index], sample_vs_label[test_index]\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % \\\n",
    "                    (train_labels.shape[0] - batch_size)\n",
    "\n",
    "            batch_data = train_dataset[offset:offset+batch_size, :]\n",
    "            batch_labels = train_labels[offset:offset+batch_size, :]\n",
    "\n",
    "            feed_dict = { tf_test_data : test_dataset, tf_train_dataset : batch_data, \\\n",
    "                         tf_train_labels : batch_labels }\n",
    "            _, l, predictions = session.run([optimizer, loss, \\\n",
    "                                             train_prediction], feed_dict=feed_dict)\n",
    "#             if step%int(num_steps/2) == 0:\n",
    "#                 print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "#                 print(\"Minibatch accuracy: %.1f%% \" % accuracy(predictions, batch_labels))\n",
    "#                 print(\"Test accuracy: %.1f%% \" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "        result[i] = accuracy(test_prediction.eval(feed_dict={tf_test_data : test_dataset}), test_labels)\n",
    "        print(\"Test accuracy: %.1f%%\" % result[i])\n",
    "        i+=1\n",
    "    print(np.mean(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
