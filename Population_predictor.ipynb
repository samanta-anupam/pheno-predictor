{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Population predictor\n",
    "=============\n",
    "\n",
    "Using Deep Learning\n",
    "------------\n",
    "\n",
    "The following things are being done here.\n",
    "1. Load dataset.npy file for the TPM count from quant.sf file. If not present:\n",
    "    a. Read from the quant.sf files to a numpy array dataset.\n",
    "    b. Save it using numpy.save file for faster access.\n",
    "2. Perform Dimensionality reduction. Possible methods:\n",
    "    a. PCA (Problems: Not for dataset with number of features larger than number of samples).\n",
    "    b. TSNE (Slow)\n",
    "    c. Encoder (Deep learning approach to encode into a lower dimensional form)\n",
    "3. Apply the model to this reduced feature vector. Possible models:\n",
    "    a. Fully connected neural network with softmax loss.\n",
    "    b. Conv1D\n",
    "    c. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the input from the quant.sf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 2 2 4 3 0 3 4 3 3 1 1 3 3 0 0 2 2 3 4 0 3 3 1 2 2 1 4 1 2 4 3 1 1 0 0\n",
      " 3 2 3 3 4 4 4 0 1 4 0 1 0 1 2 3 1 2 1 1 3 0 3 3 4 1 0 3 4 1 2 0 0 1 2 2 1\n",
      " 2 1 0 2 2 2 2 3 2 3 2 4 4 0 4 2 2 3 4 0 0 3 2 0 1 1 3 0 2 3 4 1 1 0 1 1 4\n",
      " 2 0 0 4 4 3 4 4 4 1 0 0 4 2 2 0 2 4 0 0 2 0 3 2 2 4 0 4 3 1 0 1 4 0 1 0 4\n",
      " 1 0 3 2 2 1 0 3 4 0 2 4 3 2 0 0 3 4 1 1 4 4 4 0 0 0 3 1 0 0 2 2 3 4 4 2 4\n",
      " 4 1 2 2 3 2 0 1 0 4 3 0 2 3 3 3 2 0 1 0 0 1 0 2 1 1 3 0 4 4 0 4 0 2 1 2 4\n",
      " 1 4 0 3 1 2 3 3 4 1 3 4 2 4 1 1 0 1 4 1 4 4 4 4 4 4 4 2 1 3 3 1 0 2 2 1 2\n",
      " 0 3 1 3 3 1 0 0 0 4 2 3 2 4 2 2 1 3 3 0 1 2 3 1 2 2 2 4 3 0 1 3 1 1 3 4 3\n",
      " 2 3 3 1 0 0 0 3 3 1 4 0 4 1 0 3 4 1 1 1 3 1 4 3 2 0 4 4 2 0 4 3 3 0 3 4 1\n",
      " 2 4 1 3 1 2 4 1 2 0 3 3 3 4 4 2 4 2 4 3 2 3 4 1 0 3 4 3 3 3 4 3 1 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file to map samples vs labels.\n",
    "data_frame = pd.read_csv('p1_train.csv')\n",
    "labels = list(set(data_frame['label']))\n",
    "\n",
    "num_of_transcripts = 199324\n",
    "num_of_samples = data_frame.shape[0]\n",
    "num_labels = len(labels)\n",
    "\n",
    "dataset = np.empty((num_of_samples, num_of_transcripts), dtype=np.float32)\n",
    "sample_vs_label = np.empty(num_of_samples, dtype='int')\n",
    "samples = np.empty(num_of_samples, dtype='<U9')\n",
    "\n",
    "for index, row in data_frame.iterrows():\n",
    "    if(index >= num_of_samples): break\n",
    "    sample_vs_label[index] = labels.index(row['label'])\n",
    "    samples[index] = row['accession']\n",
    "print(sample_vs_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and read from any saved 'dataset.npy' file. If present then load it to numpy array dataset, else just read it from all possible folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.67 ms, sys: 228 ms, total: 230 ms\n",
      "Wall time: 241 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if(os.path.isfile('dataset.npy')):\n",
    "    dataset = np.load('dataset.npy')\n",
    "else:\n",
    "    TRAIN_PATH = './train/'\n",
    "    # Read the 'quant.sf' value here for each sample.\n",
    "    for i in range(num_of_samples):\n",
    "        if i%20==0: print(i)\n",
    "        file_name = TRAIN_PATH + samples[i] + '/bias/quant.sf'\n",
    "        quant_sf = np.genfromtxt(file_name, delimiter='\\t', usecols=3, skip_header=True, dtype=np.float32)\n",
    "        dataset[i] = quant_sf\n",
    "\n",
    "    # Using https://i.stack.imgur.com/4d6yo.png to judge the best way to save the dataset as a npy file\n",
    "    # for faster reloading of numpy array.\n",
    "    np.save('dataset.npy', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_dataset = StandardScaler().fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97903007"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(std_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying PCA\n",
    "## results = Test accuracy: 42.1%\n",
    "\n",
    "Problem: Cannot reduce to dimensions greater than number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8750287e0e8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_of_transcripts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_of_transcripts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdataset_reduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anupam/anaconda/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anupam/anaconda/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;31m# Call different fits for either full or truncated SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msvd_solver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'full'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msvd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'arpack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'randomized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anupam/anaconda/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0;31m# flip eigenvectors' sign to enforce deterministic output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd_flip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anupam/anaconda/lib/python3.6/site-packages/scipy/linalg/decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# perform decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n\u001b[0;32m--> 116\u001b[0;31m                           full_matrices=full_matrices, overwrite_a=overwrite_a)\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "num_of_transcripts = 300\n",
    "pca = PCA(n_components=num_of_transcripts)\n",
    "pca.fit(std_dataset)\n",
    "dataset_reduced = pca.transform(std_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color_mapping = [sns.xkcd_rgb['blue'], sns.xkcd_rgb['lime'], sns.xkcd_rgb['ochre'], \n",
    "                 sns.xkcd_rgb['red'], sns.xkcd_rgb['green']]\n",
    "\n",
    "colors = [color_mapping[x] for x in sample_vs_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_reduced' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-046757cb1395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_reduced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_reduced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_reduced' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "plt.scatter(dataset_reduced[:, 0], dataset_reduced[:, 10], c=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.41663002e-02,   0.00000000e+00,   3.10742998e+00, ...,\n",
       "         4.31414014e+03,   4.41022003e+02,   2.86663989e+03], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying TSNE\n",
    "## (TODO)\n",
    "Under construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = TSNE(learning_rate=100, n_components=20, random_state=0, perplexity=10)\n",
    "tsne5 = model.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying AutoEncoder\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main modeling\n",
    "## model data to machine learning tools\n",
    "Split data to different datasets, namely, train, validation, test. Currently splitting in 0.8, 0.1, 0.1 ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (295, 300) (295,)\n",
      "Validation: (36, 300) (36,)\n",
      "Testing: (38, 300) (38,)\n"
     ]
    }
   ],
   "source": [
    "def merge_datasets(dataset, train_size, valid_size, test_size):\n",
    "  valid_dataset, valid_labels = dataset[:valid_size], sample_vs_label[:valid_size]\n",
    "  train_dataset, train_labels = dataset[valid_size:valid_size+train_size], sample_vs_label[valid_size:valid_size+train_size]\n",
    "  test_dataset, test_labels = dataset[valid_size+train_size:], sample_vs_label[valid_size+train_size:]\n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels, test_dataset, test_labels\n",
    "\n",
    "train_size = int(0.8*num_of_samples)\n",
    "valid_size = int(0.1*num_of_samples)\n",
    "test_size = num_of_samples-valid_size-train_size\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels, test_dataset, test_labels = merge_datasets(dataset_reduced, train_size, valid_size, test_size)\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use One Hot encoding for the labels here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "output_extras": [
      {
       "item_id": 1.0
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11952.0,
     "status": "ok",
     "timestamp": 1.446658914857E12,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480.0
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (295, 300) (295, 5)\n",
      "Validation set (36, 300) (36, 5)\n",
      "Test set (38, 300) (38, 5)\n"
     ]
    }
   ],
   "source": [
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, dataset.shape[1])).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369, 300)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "## FC Connected layer\n",
    "Using a 2-layer fully connected neural network of 128, 64 relu neurons with batch size of 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 128)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_hidden_nodes = [128, 64]\n",
    "input_size = [num_of_transcripts]+num_hidden_nodes\n",
    "num_layers = len(num_hidden_nodes)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=\n",
    "                                     (batch_size, num_of_transcripts))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=\n",
    "                                    (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    # Weights\n",
    "    weights1 = []\n",
    "    biases1 = []\n",
    "    beta1 = []\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        weights1.append(tf.Variable(tf.truncated_normal([input_size[i], num_hidden_nodes[i]],\\\n",
    "                                                        stddev=np.sqrt(2.0 / (input_size[i])))))\n",
    "        biases1.append(tf.Variable(tf.zeros([num_hidden_nodes[i]])))\n",
    "        beta1.append(1e-3)\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([input_size[-1], num_labels],\\\n",
    "                                               stddev=np.sqrt(2.0 / input_size[-1])))\n",
    "    beta2 = 1e-3\n",
    "    \n",
    "    # Training Computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1[0]) + biases1[0])\n",
    "    drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "    for i in range(1, num_layers):\n",
    "        print(lay1_train.shape)\n",
    "        lay1_train = tf.nn.relu(tf.matmul(drop1, weights1[i]) + biases1[i])\n",
    "        drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "    logits = tf.matmul(drop1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\\\n",
    "                           + beta2 * tf.nn.l2_loss(weights2)\n",
    "    for i in range(num_layers):\n",
    "        loss += beta1[i] * tf.nn.l2_loss(weights1[i]) \n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate = tf.train.exponential_decay(1e-4, global_step, 1000000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "                           \n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1[0]) + biases1[0])\n",
    "    for i in range(1, num_layers):\n",
    "        lay1_valid = tf.nn.relu(tf.matmul(lay1_valid, weights1[i]) + biases1[i])\n",
    "    logits_valid = tf.matmul(lay1_valid, weights2) + biases2\n",
    "    valid_prediction = tf.nn.softmax(logits_valid)\n",
    "                           \n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1[0]) + biases1[0])\n",
    "    for i in range(1, num_layers):\n",
    "        lay1_test = tf.nn.relu(tf.matmul(lay1_test, weights1[i]) + biases1[i])\n",
    "    logits_test = tf.matmul(lay1_test, weights2) + biases2\n",
    "    test_prediction = tf.nn.softmax(logits_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the neural net\n",
    "## Training for 18001 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised\n",
      "Minibatch loss at step 0: 2314.843750\n",
      "Minibatch accuracy: 37.5% \n",
      "Validation accuracy: 27.8% \n",
      "Minibatch loss at step 500: 1575.408203\n",
      "Minibatch accuracy: 12.5% \n",
      "Validation accuracy: 27.8% \n",
      "Minibatch loss at step 1000: 542.723389\n",
      "Minibatch accuracy: 50.0% \n",
      "Validation accuracy: 41.7% \n",
      "Minibatch loss at step 1500: 274.559753\n",
      "Minibatch accuracy: 50.0% \n",
      "Validation accuracy: 44.4% \n",
      "Minibatch loss at step 2000: 96.578377\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 52.8% \n",
      "Minibatch loss at step 2500: 198.863541\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 50.0% \n",
      "Minibatch loss at step 3000: 180.029572\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 44.4% \n",
      "Minibatch loss at step 3500: 748.424683\n",
      "Minibatch accuracy: 50.0% \n",
      "Validation accuracy: 44.4% \n",
      "Minibatch loss at step 4000: 66.058975\n",
      "Minibatch accuracy: 50.0% \n",
      "Validation accuracy: 47.2% \n",
      "Minibatch loss at step 4500: 32.389664\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 47.2% \n",
      "Minibatch loss at step 5000: 50.274273\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 47.2% \n",
      "Minibatch loss at step 5500: 160.247375\n",
      "Minibatch accuracy: 37.5% \n",
      "Validation accuracy: 47.2% \n",
      "Minibatch loss at step 6000: 8.401417\n",
      "Minibatch accuracy: 87.5% \n",
      "Validation accuracy: 50.0% \n",
      "Minibatch loss at step 6500: 18.715597\n",
      "Minibatch accuracy: 75.0% \n",
      "Validation accuracy: 47.2% \n",
      "Minibatch loss at step 7000: 47.475883\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 50.0% \n",
      "Minibatch loss at step 7500: 28.758068\n",
      "Minibatch accuracy: 75.0% \n",
      "Validation accuracy: 50.0% \n",
      "Minibatch loss at step 8000: 56.231724\n",
      "Minibatch accuracy: 50.0% \n",
      "Validation accuracy: 44.4% \n",
      "Minibatch loss at step 8500: 14.043343\n",
      "Minibatch accuracy: 50.0% \n",
      "Validation accuracy: 41.7% \n",
      "Minibatch loss at step 9000: 64.627083\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 38.9% \n",
      "Minibatch loss at step 9500: 2.416763\n",
      "Minibatch accuracy: 37.5% \n",
      "Validation accuracy: 41.7% \n",
      "Minibatch loss at step 10000: 15.375936\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 33.3% \n",
      "Minibatch loss at step 10500: 4.956680\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 41.7% \n",
      "Minibatch loss at step 11000: 9.931083\n",
      "Minibatch accuracy: 37.5% \n",
      "Validation accuracy: 38.9% \n",
      "Minibatch loss at step 11500: 7.368093\n",
      "Minibatch accuracy: 50.0% \n",
      "Validation accuracy: 33.3% \n",
      "Minibatch loss at step 12000: 1.686150\n",
      "Minibatch accuracy: 25.0% \n",
      "Validation accuracy: 36.1% \n",
      "Minibatch loss at step 12500: 5.281987\n",
      "Minibatch accuracy: 50.0% \n",
      "Validation accuracy: 38.9% \n",
      "Minibatch loss at step 13000: 2.294142\n",
      "Minibatch accuracy: 37.5% \n",
      "Validation accuracy: 33.3% \n",
      "Minibatch loss at step 13500: 1.611529\n",
      "Minibatch accuracy: 25.0% \n",
      "Validation accuracy: 36.1% \n",
      "Minibatch loss at step 14000: 0.953768\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 36.1% \n",
      "Minibatch loss at step 14500: 1.466639\n",
      "Minibatch accuracy: 25.0% \n",
      "Validation accuracy: 36.1% \n",
      "Minibatch loss at step 15000: 1.491523\n",
      "Minibatch accuracy: 37.5% \n",
      "Validation accuracy: 36.1% \n",
      "Minibatch loss at step 15500: 0.987989\n",
      "Minibatch accuracy: 75.0% \n",
      "Validation accuracy: 36.1% \n",
      "Minibatch loss at step 16000: 1.267267\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 36.1% \n",
      "Minibatch loss at step 16500: 3.982396\n",
      "Minibatch accuracy: 37.5% \n",
      "Validation accuracy: 36.1% \n",
      "Minibatch loss at step 17000: 2.168455\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 36.1% \n",
      "Minibatch loss at step 17500: 1.575858\n",
      "Minibatch accuracy: 25.0% \n",
      "Validation accuracy: 36.1% \n",
      "Minibatch loss at step 18000: 1.893598\n",
      "Minibatch accuracy: 62.5% \n",
      "Validation accuracy: 36.1% \n",
      "Test accuracy: 42.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "accuracy_val = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialised')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % \\\n",
    "                (train_labels.shape[0] - batch_size)\n",
    "\n",
    "        batch_data = train_dataset[offset:offset+batch_size, :]\n",
    "        batch_labels = train_labels[offset:offset+batch_size, :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset : batch_data, \\\n",
    "                     tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, \\\n",
    "                                         train_prediction], feed_dict=feed_dict)\n",
    "        if step%500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%% \" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%% \" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
